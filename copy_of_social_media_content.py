# -*- coding: utf-8 -*-
"""Copy of Social Media Content.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13u1YOShd1XLQy7zTti7ZXNyY-OxaxA6Y
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nltk
!pip install sns

import numpy as np
import pandas as pd
import seaborn as sns 
import nltk
#nltk.download('punkt')
#nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from pandas.core.common import random_state
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

tds = pd.read_csv("/content/drive/MyDrive/Major project/True.csv")
fds = pd.read_csv("/content/drive/MyDrive/Major project/Fake.csv")

tds

class_true = ["Real"]*21417
tds["class_label"] = class_true
tds

fds

class_fake = ["Fake"]*23481
fds["class_label"] = class_fake
fds

# Preprocessing

tweets_dataset = pd.concat([tds,fds], axis=0)
index = [i for i in range(len(tweets_dataset))]
tweets_dataset["index"] = index
tweets_dataset.set_index('index',inplace=True)

import string

def punctuation_removal(text):
    all_list = [char for char in text if char not in string.punctuation]
    clean_str = ''.join(all_list)
    return clean_str

tweets_dataset['text'] = tweets_dataset['text'].apply(punctuation_removal)

tweets_dataset

# converting to all lowercase
tweets_dataset['text'] = tweets_dataset['text'].apply(lambda x: x.lower())
tweets_dataset.head()

# Removing stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

tweets_dataset['text'] = tweets_dataset['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#shuffled dataset
tweets_dataset = tweets_dataset.sample(frac=1)
tweets_dataset

tweets_dataset.class_label.value_counts()

tweets_dataset["class_label_num"] = tweets_dataset["class_label"].map({"Fake":0,"Real":1})
tweets_dataset

"""## Exploration & Data Analysis"""

# How many articles per subject?
print(tweets_dataset.groupby(['subject'])['text'].count())
tweets_dataset.groupby(['subject'])['text'].count().plot(kind="bar")
plt.show()

# How many fake and real articles?
print(tweets_dataset.groupby(['class_label'])['text'].count())
tweets_dataset.groupby(['class_label'])['text'].count().plot(kind="bar")
plt.show()

# Word cloud for fake news
from wordcloud import WordCloud

fake_data = tweets_dataset[tweets_dataset["class_label"] == "Fake"]
all_words = ' '.join([text for text in fake_data.text])

wordcloud = WordCloud(width= 800, height= 500,
                          max_font_size = 110,
                          collocations = False).generate(all_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Word cloud for real news
from wordcloud import WordCloud

fake_data = tweets_dataset[tweets_dataset["class_label"] == "Real"]
all_words = ' '.join([text for text in fake_data.text])

wordcloud = WordCloud(width= 800, height= 500,
                          max_font_size = 110,
                          collocations = False).generate(all_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Most frequent words counter (Code adapted from https://www.kaggle.com/rodolfoluna/fake-news-detector)   
from nltk import tokenize

token_space = tokenize.WhitespaceTokenizer()

def counter(text, column_text, quantity):
    all_words = ' '.join([text for text in text[column_text]])
    token_phrase = token_space.tokenize(all_words)
    frequency = nltk.FreqDist(token_phrase)
    df_frequency = pd.DataFrame({"Word": list(frequency.keys()),
                                   "Frequency": list(frequency.values())})
    df_frequency = df_frequency.nlargest(columns = "Frequency", n = quantity)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data = df_frequency, x = "Word", y = "Frequency", color = 'blue')
    ax.set(ylabel = "Count")
    plt.xticks(rotation='vertical')
    plt.show()

# Most frequent words in fake news
counter(tweets_dataset[tweets_dataset["class_label"] == "Fake"], "text", 20)

# Most frequent words in real news
counter(tweets_dataset[tweets_dataset["class_label"] == "Real"], "text", 20)



!pip install spacy

import spacy
spacy.cli.download("en_core_web_lg")
nlp = spacy.load("en_core_web_lg", disable=['ner', 'parser'])

#nlp("Rohit Sharma's Team India will face Jos Buttler-led England in the second semifinal of the ICC men’s T20 World Cup 2022 at the Adelaide Oval on Thursday (November 10).\n Fans are hoping for a India vs Pakistan final, with Babar Azam's side already in the final after win over New Zealand on Wednesday (November 9). \nRohit Sharma’s side topped Super 12 Group 2 table with four wins in five games, their only loss coming against South Africa at Perth.\n\nEngland, on the other hand, finished second in Super 12 Group 1, losing to Ireland and their game against Australia was washed out without a ball being bowled.").vector

tweets_dataset["text_vec"]=tweets_dataset["text"].apply(lambda text: nlp(text).vector)
tweets_dataset

tweets_dataset["text_vec"]

# tweets_dataset["text_vec"].to_csv("/content/drive/MyDrive/Fake_News_Detection/TweetsDataset.csv", index=False)
# tweetdata = pd.read_csv("/content/drive/MyDrive/Fake_News_Detection/TweetsDataset.csv")
# tweetdata["text_vec"][0]

tweet_train, tweet_test, label_train, label_test = train_test_split(tweets_dataset.text_vec.values, tweets_dataset.class_label_num, test_size=0.2, random_state=2022)

tweet_train_2D = np.stack(tweet_train)
tweet_test_2D = np.stack(tweet_test)

"""
https://www.researchgate.net/post/what_are_the_different_algorithms_for_text_classification

Some Traditional ML Algorithms:
Logisitic Regression, Multinomial Naive Bayes, k Nearest Neighbors, Decision Tree and Support Vector Machine
Some Ensemble ML Algorithms are:
Adaboost, Random Forest, Bagging, Gradient Boosting etc
deep learning ones: 
Convolutional Neural Network (CNN), Long Short Term Modelr (LSTM), Recurrent Convolutional Neural Network (RCNN), etc
"""

models= dict()

mnb = MultinomialNB()
scaler = MinMaxScaler()
scaled_train = scaler.fit_transform(tweet_train_2D) #Negative values in data passed to MultinomialNB (input X)
scaled_test = scaler.transform(tweet_test_2D)
mnb.fit(scaled_train, label_train) # scaled to get positive range of numbers for pos & neg data

label_pred = mnb.predict(scaled_test)
print(classification_report(label_test, label_pred))
print("accuracy: {}%".format(round(accuracy_score(label_test, label_pred)*100,2)))
models['MultinomialNB()']=round(accuracy_score(label_test, label_pred)*100,2)

confusion_matrix_nb = confusion_matrix(label_test, label_pred)

cm_display= ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_nb, display_labels = [False, True])

cm_display.plot()
plt.show()

knn = KNeighborsClassifier(n_neighbors = 5, metric = "euclidean")
knn.fit(tweet_train_2D, label_train)

label_pred_knn = knn.predict(tweet_test_2D)
print(classification_report(label_test, label_pred_knn))
print("accuracy: {}%".format(round(accuracy_score(label_test, label_pred_knn)*100,2)))
models['KNN']=round(accuracy_score(label_test, label_pred_knn)*100,2)

confusion_matrix_knn = confusion_matrix(label_test, label_pred_knn)
cm_display= ConfusionMatrixDisplay(confusion_matrix_knn ,display_labels = [False, True])
cm_display.plot()
plt.show()

sv = svm.SVC()
sv.fit(tweet_train_2D, label_train)

label_pred_sv = sv.predict(tweet_test_2D)
print(classification_report(label_test, label_pred_sv))
print("accuracy: {}%".format(round(accuracy_score(label_test, label_pred_sv)*100,2)))
models['SVM']=round(accuracy_score(label_test, label_pred_sv)*100,2)

confusion_matrix_sv = confusion_matrix(label_test, label_pred_sv)

cm_display= ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_sv, display_labels = [False, True])

cm_display.plot()
plt.show()

randfor = RandomForestClassifier(max_depth=14, random_state=2022) # at max_depth 14, acc is max, no inc after that
#The random_state in these algorithms controls two randomized processes — bootstrapping of the samples when creating tress and getting a random subset of features to search for the best feature during the node splitting process when creating each tree.
randfor.fit(tweet_train_2D, label_train)

label_pred_randfor = randfor.predict(tweet_test_2D)
print(classification_report(label_test, label_pred_randfor))
print("accuracy: {}%".format(round(accuracy_score(label_test, label_pred_randfor)*100,2)))
models['Random Forest']=round(accuracy_score(label_test, label_pred_randfor)*100,2)

confusion_matrix_rd = confusion_matrix(label_test, label_pred_randfor)
cm_display= ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_rd, display_labels = [False, True])
cm_display.plot()
plt.show()

plt.figure(figsize=(8,7))
plt.bar(list(models.keys()),list(models.values()))
plt.ylim(80,100)
plt.ylabel('Model Accuracy')
plt.title('Comparison of Accuracy of models')
plt.show()